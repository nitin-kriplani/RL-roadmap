{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e69073",
   "metadata": {},
   "source": [
    "# Markov Decision Processes \n",
    "This notebook reproduces the key concepts and the \"Student\" examples from David Silver's Lecture 2.\n",
    "\n",
    "Sections:\n",
    "1. Markov Processes (MP)\n",
    "2. Markov Reward Processes (MRP)\n",
    "3. Markov Decision Processes (MDP)\n",
    "\n",
    "Run cells in order. Set kernel to Python 3 if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2da7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Setup & Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00698338",
   "metadata": {},
   "source": [
    "## Section 1: Markov Chain (MP)\n",
    "\n",
    "A **Markov Process** (or **Markov Chain**) is a tuple ‚ü®ùíÆ, ‚Ñô‚ü©:\n",
    "\n",
    "- **ùíÆ** is a (finite) set of states  \n",
    "- **‚Ñô** is a state transition probability matrix,  \n",
    "  ‚Ñô‚Çõ‚Çõ‚Ä≤ = ‚Ñô[ùëÜ‚Çú‚Çä‚ÇÅ = s‚Ä≤ | ùëÜ‚Çú = s]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c90c6",
   "metadata": {},
   "source": [
    "### Student Example\n",
    "This section defines the Student Markov Chain transition matrix and demonstrates sampling episodes (random walk).\n",
    "\n",
    "![alt text](images/student_example_1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Transition Matrix P:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class 1</th>\n",
       "      <th>Class 2</th>\n",
       "      <th>Class 3</th>\n",
       "      <th>Pass</th>\n",
       "      <th>Pub</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>Sleep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pass</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pub</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sleep</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Class 1  Class 2  Class 3  Pass  Pub  Facebook  Sleep\n",
       "Class 1       0.0      0.5      0.0   0.0  0.0       0.5    0.0\n",
       "Class 2       0.0      0.0      0.8   0.0  0.0       0.0    0.2\n",
       "Class 3       0.0      0.0      0.0   0.6  0.4       0.0    0.0\n",
       "Pass          0.0      0.0      0.0   0.0  0.0       0.0    1.0\n",
       "Pub           0.2      0.4      0.4   0.0  0.0       0.0    0.0\n",
       "Facebook      0.1      0.0      0.0   0.0  0.0       0.9    0.0\n",
       "Sleep         0.0      0.0      0.0   0.0  0.0       0.0    1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row sums (should be 1.0 for each state):\n",
      "Class 1     1.0\n",
      "Class 2     1.0\n",
      "Class 3     1.0\n",
      "Pass        1.0\n",
      "Pub         1.0\n",
      "Facebook    1.0\n",
      "Sleep       1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define states and transition matrix P\n",
    "states = [\"Class 1\", \"Class 2\", \"Class 3\", \"Pass\", \"Pub\", \"Facebook\", \"Sleep\"]\n",
    "n_states = len(states)\n",
    "\n",
    "P = np.array([\n",
    "    [0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0],  # Class 1\n",
    "    [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.2],  # Class 2\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.4, 0.0, 0.0],  # Class 3\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # Pass\n",
    "    [0.2, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0],  # Pub\n",
    "    [0.1, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0],  # Facebook\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   # Sleep\n",
    "])\n",
    "\n",
    "# Display\n",
    "print(\"State Transition Matrix P:\")\n",
    "df_P = pd.DataFrame(P, index=states, columns=states)\n",
    "display(df_P)\n",
    "\n",
    "# Validate rows sum to 1\n",
    "row_sums = P.sum(axis=1)\n",
    "print(\"\\nRow sums (should be 1.0 for each state):\")\n",
    "print(pd.Series(row_sums, index=states))\n",
    "print(\"\\nAll rows sum to 1.0:\", np.allclose(row_sums, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d553837",
   "metadata": {},
   "source": [
    "## Section 2: Markov Reward Process (MRP)\n",
    "\n",
    "A **Markov Reward Process** is a tuple ‚ü®ùíÆ, ‚Ñô, ‚Ñõ, Œ≥‚ü©:\n",
    "\n",
    "- **ùíÆ** is a finite set of states  \n",
    "- **‚Ñô** is a state transition probability matrix,  \n",
    "  ‚Ñô‚Çõ‚Çõ‚Ä≤ = ‚Ñô[ùëÜ‚Çú‚Çä‚ÇÅ = s‚Ä≤ | ùëÜ‚Çú = s]  \n",
    "- **‚Ñõ** is a reward function,  \n",
    "  ‚Ñõ‚Çõ = ùîº[ùëÖ‚Çú‚Çä‚ÇÅ | ùëÜ‚Çú = s]  \n",
    "- **Œ≥** is a discount factor, Œ≥ ‚àà [0, 1]\n",
    "\n",
    "Define reward vector R and solve v = (I - gamma P)^{-1} R for several gamma values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad5dd3",
   "metadata": {},
   "source": [
    "## Value Function\n",
    "\n",
    "The **value function** $v(s)$ gives the long-term value of state $s$.\n",
    "\n",
    "The **state value function** $v(s)$ of a Markov Reward Process (MRP) is the expected return starting from state $s$:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "v(s) = \\mathbb{E}[G_t \\mid S_t = s]\n",
    "$$\n",
    "\n",
    "where, \n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ....\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea9bcb",
   "metadata": {},
   "source": [
    "## Bellman Equation for MRPs\n",
    "\n",
    "The value function satisfies the recursive relationship:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "v(s) = \\mathbb{E}[R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s]\n",
    "$$\n",
    "\n",
    "In matrix form, \n",
    "$$\n",
    "v = R + \\gamma Pv\n",
    "$$\n",
    " or, \n",
    " $$\n",
    " v = (I - \\gamma P)^{-1} R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56dded73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-value v(s) for different gamma values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gamma=0.0</th>\n",
       "      <th>gamma=0.9</th>\n",
       "      <th>gamma=1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>-5.012729</td>\n",
       "      <td>-9.899471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.942655</td>\n",
       "      <td>4.100529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>4.087021</td>\n",
       "      <td>6.964727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pass</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.643739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pub</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.908392</td>\n",
       "      <td>3.446208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-7.637608</td>\n",
       "      <td>-19.899471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sleep</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.643739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gamma=0.0  gamma=0.9  gamma=1.0\n",
       "Class 1        -2.0  -5.012729  -9.899471\n",
       "Class 2        -2.0   0.942655   4.100529\n",
       "Class 3        -2.0   4.087021   6.964727\n",
       "Pass           10.0  10.000000  12.643739\n",
       "Pub             1.0   1.908392   3.446208\n",
       "Facebook       -1.0  -7.637608 -19.899471\n",
       "Sleep           0.0   0.000000   2.643739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reward vector\n",
    "R = np.array([-2, -2, -2, 10, 1, -1, 0])\n",
    "\n",
    "def solve_mrp(P, R, gamma):\n",
    "    I = np.eye(len(P))\n",
    "    try:\n",
    "        v = np.linalg.inv(I - gamma * P).dot(R)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Use least-squares solver for singular matrices\n",
    "        v = np.linalg.lstsq(I - gamma * P, R, rcond=None)[0]\n",
    "    return v\n",
    "\n",
    "gammas = [0.0, 0.9, 1.0]\n",
    "results = {f'gamma={g}': solve_mrp(P, R, g) for g in gammas}\n",
    "\n",
    "df_v = pd.DataFrame(results, index=states)\n",
    "print(\"State-value v(s) for different gamma values:\")\n",
    "display(df_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93ce87",
   "metadata": {},
   "source": [
    "## Section 3: Markov Decision Process (MDP)\n",
    "\n",
    "A **Markov Decision Process (MDP)** is a Markov Reward Process with decisions. It models an environment where all states satisfy the Markov property.\n",
    "\n",
    "An MDP is defined as a tuple ‚ü®ùíÆ, ùíú, ‚Ñô, ‚Ñõ, Œ≥‚ü©:\n",
    "\n",
    "- **ùíÆ**: finite set of states  \n",
    "- **ùíú**: finite set of actions  \n",
    "- **‚Ñô**: state transition probability matrix  \n",
    "  \n",
    "\n",
    "$$\n",
    "  \\mathcal{P}_{ss'}^a = \\mathbb{P}[S_{t+1} = s' \\mid S_t = s, A_t = a]\n",
    "  $$\n",
    "\n",
    "\n",
    "- **‚Ñõ**: reward function  \n",
    "  \n",
    "\n",
    "$$\n",
    "  \\mathcal{R}_s^a = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t = a]\n",
    "  $$\n",
    "\n",
    "\n",
    "- **Œ≥**: discount factor, Œ≥ ‚àà [0, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad35fee",
   "metadata": {},
   "source": [
    "### Section 4.1: Student MDP (Actions & Transitions)\n",
    "\n",
    "We encode transitions as transitions[s][a] = [(prob, next_state, reward), ...].\n",
    "This model follows the lecture diagram: Study, Facebook, Sleep, Pub actions as appropriate.\n",
    "\n",
    "*Action Set:* **ùíú** = [Facebook, Quit, Study, Sleep, Pub]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b122e2",
   "metadata": {},
   "source": [
    "![alt text](images/student_example_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45479064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP transitions defined and validated.\n"
     ]
    }
   ],
   "source": [
    "# Define MDP transitions\n",
    "# States: 0:C1, 1:C2, 2:C3, 3:Pass, 4:Pub(not used as state-action), 5:FB, 6:Sleep\n",
    "transitions = {\n",
    "    0: {  # Class 1\n",
    "        'Study': [(1.0, 1, -2)],\n",
    "        'Facebook': [(1.0, 5, -1)]\n",
    "    },\n",
    "    1: {  # Class 2\n",
    "        'Study': [(1.0, 2, -2)],\n",
    "        'Sleep': [(1.0, 6, 0)]\n",
    "    },\n",
    "    2: {  # Class 3\n",
    "        'Study': [(1.0, 3, 10)],\n",
    "        'Pub': [(0.2, 0, 1), (0.4, 1, 1), (0.4, 2, 1)]\n",
    "    },\n",
    "    3: {  # Pass\n",
    "        'Sleep': [(1.0, 6, 0)]\n",
    "    },\n",
    "    5: {  # Facebook state\n",
    "        'Facebook': [(1.0, 5, -1)],\n",
    "        'Quit': [(1.0, 0, 0)]\n",
    "    },\n",
    "    6: {  # Sleep terminal\n",
    "        'Sleep': [(1.0, 6, 0)]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Basic validation of transition probability distributions\n",
    "for s, acts in transitions.items():\n",
    "    for a, outs in acts.items():\n",
    "        probs = [p for p,_,_ in outs]\n",
    "        if not np.isclose(sum(probs), 1.0):\n",
    "            print(f\"Warning: probabilities for state {states[s]}, action {a} sum to {sum(probs)}\")\n",
    "print(\"MDP transitions defined and validated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c095f6",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "A **policy** $ \\pi $ is a distribution over actions given states:\n",
    "\n",
    "$$\n",
    "\\pi(a \\mid s) = \\mathbb{P}[A_t = a \\mid S_t = s]\n",
    "$$\n",
    "\n",
    "- A policy fully defines the behaviour of an agent  \n",
    "- MDP policies depend only on the current state (not the history)  \n",
    "- Policies are **stationary** (time-independent):  \n",
    "  $$\n",
    "  A_t \\sim \\pi(\\cdot \\mid S_t), \\quad \\forall t > 0\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7be7b6",
   "metadata": {},
   "source": [
    "### State-Value Function\n",
    "\n",
    "The **state-value function** $v_{\\pi}(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\\pi$:\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi} \\left[ G_t \\mid S_t = s \\right]\n",
    "$$\n",
    "\n",
    "### Action-Value Function\n",
    "\n",
    "The **action-value function** $q_{\\pi}(s, a)$ is the expected return starting from state $s$, taking action $a$, and then following policy $\\pi$:\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\mathbb{E}_{\\pi} \\left[ G_t \\mid S_t = s, A_t = a \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "### Optimal Value Function\n",
    "\n",
    "$$\n",
    "v_*(s) = max_\\pi \\{v_{\\pi}(s)\\}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "q_*(s,a) = max_\\pi \\{q_{\\pi}(s,a)\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6d82ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal state values v*(s):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v*(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pass</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pub</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook</th>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sleep</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          v*(s)\n",
       "Class 1     6.0\n",
       "Class 2     8.0\n",
       "Class 3    10.0\n",
       "Pass        0.0\n",
       "Pub         0.0\n",
       "Facebook    6.0\n",
       "Sleep       0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy policy from v*:\n",
      "  Class 1: Study\n",
      "  Class 2: Study\n",
      "  Class 3: Study\n",
      "  Pass: Sleep\n",
      "  Pub: None\n",
      "  Facebook: Quit\n",
      "  Sleep: Sleep\n"
     ]
    }
   ],
   "source": [
    "# Value Iteration Algorithm (will be discussed in the week 2 materials)\n",
    "\n",
    "def value_iteration(transitions, n_states, gamma=1.0, theta=1e-6, max_iters=10000):\n",
    "    V = np.zeros(n_states)\n",
    "    for it in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(n_states):\n",
    "            if s not in transitions:\n",
    "                continue\n",
    "            v = V[s]\n",
    "            action_vals = []\n",
    "            for a, outcomes in transitions[s].items():\n",
    "                q = 0.0\n",
    "                for prob, s_next, r in outcomes:\n",
    "                    q += prob * (r + gamma * V[s_next])\n",
    "                action_vals.append(q)\n",
    "            V[s] = max(action_vals) if action_vals else 0.0\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "V_star = value_iteration(transitions, n_states, gamma=1.0)\n",
    "print(\"Optimal state values v*(s):\")\n",
    "display(pd.DataFrame(V_star, index=states, columns=[\"v*(s)\"]))\n",
    "\n",
    "# Extract greedy policy\n",
    "policy = {}\n",
    "for s in range(n_states):\n",
    "    if s not in transitions:\n",
    "        policy[states[s]] = None\n",
    "        continue\n",
    "    best_a = None\n",
    "    best_q = -np.inf\n",
    "    for a, outcomes in transitions[s].items():\n",
    "        q = sum(prob * (r + 1.0 * V_star[s_next]) for prob, s_next, r in outcomes)\n",
    "        if q > best_q:\n",
    "            best_q = q\n",
    "            best_a = a\n",
    "    policy[states[s]] = best_a\n",
    "\n",
    "print(\"Greedy policy from v*:\")\n",
    "for s, a in policy.items():\n",
    "    print(f\"  {s}: {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef7a7d",
   "metadata": {},
   "source": [
    "## Closing notes & exercises\n",
    "\n",
    "- Try modifying rewards and see how policies change.\n",
    "- Add stochasticity to Study actions and examine robustness.\n",
    "- Exercise: implement prioritized sweeping or asynchronous value iteration on this MDP.\n",
    "\n",
    "---\n",
    "\n",
    "End of notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
